{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3943d54",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f878d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "label_col = \" Label\"\n",
    "variables = {\n",
    "    \"n_estimators\": [10, 12, 14, 16],\n",
    "    \"classes\": [\n",
    "        [\"DrDoS_DNS\", \"BENIGN\"],\n",
    "        [\"DrDoS_NetBIOS\", \"DrDoS_SSDP\", \"TFTP\"],\n",
    "        [\"DrDoS_LDAP\", \"DrDoS_DNS\", \"DrDoS_NTP\", \"DrDoS_MSSQL\"],\n",
    "    ],\n",
    "    \"samples\": [1000, 2000, 3000, 4000],\n",
    "}\n",
    "\n",
    "scenarios = [\n",
    "    {\"n_estimators\": estimators, \"classes\": classes, \"samples\": samples}\n",
    "    for estimators, classes, samples in itertools.product(\n",
    "        variables[\"n_estimators\"], variables[\"classes\"], variables[\"samples\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, scenario in enumerate(scenarios):\n",
    "    print(f'Scenario {i + 1}:')\n",
    "    print(f'    Classes: {', '.join(scenario['classes'])}')\n",
    "    print(f'    Samples: {scenario['samples']}')\n",
    "    print(f'    N Estimators: {scenario['n_estimators']}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56064f63",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = pd.read_csv('datasets/Benign.csv')\n",
    "benign.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "benign.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802624d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dos = pd.read_csv(\"datasets/NDDoS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dos, benign], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701a7a1",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpn_tree.cpn_tree import CPNTree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_scenario(scenario):\n",
    "    path = Path(\n",
    "        f\"out/{'-'.join(scenario['classes'])}/{scenario['n_estimators']}_estimators/{scenario['samples']}_samples/\"\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n_classes = len(scenario[\"classes\"])\n",
    "    base_n = scenario[\"samples\"] // n_classes\n",
    "    remainder = scenario[\"samples\"] % n_classes\n",
    "    remainder_classes = np.random.RandomState(42).choice(\n",
    "        scenario[\"classes\"], remainder, replace=False\n",
    "    )\n",
    "\n",
    "    if \"BENIGN\" in scenario[\"classes\"]:\n",
    "        columns = [\n",
    "            \" Packet Length Mean\",\n",
    "            \" Subflow Fwd Bytes\",\n",
    "            \" Flow Packets/s\",\n",
    "            \" Flow IAT Mean\",\n",
    "            \" Flow Duration\",\n",
    "            \" act_data_pkt_fwd\",\n",
    "            \" Fwd Header Length\",\n",
    "            \"Init_Win_bytes_forward\",\n",
    "            \"Subflow Fwd Packets\",\n",
    "            \" Packet Length Variance\",\n",
    "            \" Packet Length Std\",\n",
    "            \" ACK Flag Count\",\n",
    "            \" min_seg_size_forward\",\n",
    "            \" Fwd Packet Length Std\",\n",
    "            \" Label\",\n",
    "        ]\n",
    "    else:\n",
    "        columns = [\n",
    "            \"Flow Bytes/s\",\n",
    "            \"Total Length of Fwd Packets\",\n",
    "            \"Fwd Packets/s\",\n",
    "            \" Flow Duration\",\n",
    "            \" Fwd Header Length\",\n",
    "            \"Subflow Fwd Packets\",\n",
    "            \" ACK Flag Count\",\n",
    "            \" min_seg_size_forward\",\n",
    "            \" Fwd IAT Min\",\n",
    "            \" Packet Length Variance\",\n",
    "            \" Packet Length Std\",\n",
    "            \" Label\",\n",
    "        ]\n",
    "    dfb = df[columns]\n",
    "    sampled = [\n",
    "        dfb[dfb[label_col] == class_].sample(\n",
    "            base_n + (1 if class_ in remainder_classes else 0), random_state=42\n",
    "        )\n",
    "        for class_ in scenario[\"classes\"]\n",
    "    ]\n",
    "    df_minor = pd.concat(sampled, ignore_index=True)\n",
    "    X, y = df_minor.drop(columns=[label_col]), df_minor[label_col]\n",
    "\n",
    "    gbm = GradientBoostingClassifier(\n",
    "        n_estimators=scenario[\"n_estimators\"], random_state=42\n",
    "    ).fit(X, y)\n",
    "    model_predictions = pd.Series(gbm.predict(X), index=X.index)\n",
    "\n",
    "    cpn = CPNTree().add_from_GradientBoostingClassifier(\"gbm\", gbm)\n",
    "    start_time = time.time()\n",
    "    net_predictions = cpn.predict(X, write_cpn=Path(path, \"net.cpn\"))\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    with open(Path(path, \"model.pkl\"), \"wb\") as f:\n",
    "        pkl.dump(model_predictions, f)\n",
    "\n",
    "    with open(Path(path, \"net.pkl\"), \"wb\") as f:\n",
    "        pkl.dump({**net_predictions, \"elapsed_time\": elapsed_time}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    path = Path(\n",
    "        f\"out/{'-'.join(scenario['classes'])}/{scenario['n_estimators']}_estimators/{scenario['samples']}_samples/\"\n",
    "    )\n",
    "    if not (Path(path, 'model.pkl').exists() and Path(path, 'net.pkl').exists() or Path(path, 'skipped').exists()): \n",
    "        print(f'Executing scenario {i} at {datetime.datetime.now()}: {scenario}')\n",
    "        execute_scenario(scenario)\n",
    "\n",
    "    if Path(path, 'model.pkl').exists() and Path(path, 'net.pkl').exists():\n",
    "        print(f\"Scenario {i} results: {scenario}\")\n",
    "        with open(Path(path, 'model.pkl'), 'rb') as f:\n",
    "            model_predictions = pkl.load(f).apply(CPNTree.format_text)\n",
    "        with open(Path(path, 'net.pkl'), \"rb\") as f:\n",
    "            net = pkl.load(f)\n",
    "            net_predictions = net['gbm']\n",
    "            elapsed_time = net['elapsed_time']\n",
    "        comparisons = model_predictions == net_predictions\n",
    "        print(f\"Matches: {len(comparisons[comparisons])}/{len(comparisons)} -> {comparisons.mean() * 100}%/100%\")\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds / {elapsed_time/60} minutes / {elapsed_time/3600} hours\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpn-tree-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
